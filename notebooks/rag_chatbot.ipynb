{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc27432",
   "metadata": {},
   "source": [
    "# <h3><b>Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3358c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "import pymupdf4llm\n",
    "import glob\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import CrossEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b7ad4",
   "metadata": {},
   "source": [
    "# <h3><b>Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d72bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "CHROMA_PERSIST_DIRECTORY = \"../chroma_db\"\n",
    "COLLECTION_NAME = \"machine_learning_modules\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22c66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8b8fd",
   "metadata": {},
   "source": [
    "# <h3><b>LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c590ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    model_name=\"openai/gpt-oss-120b\",\n",
    "    temperature=0.2\n",
    ")\n",
    "print(\"‚úÖ LLM initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0353c8",
   "metadata": {},
   "source": [
    "# <h3><b>Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38bf356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ekstrak PDF ke Markdown\n",
    "# md_text = pymupdf4llm.to_markdown(r\"D:\\Portfolio\\rag-llm-education\\data\\documents\\Machine Learning _ Model Klasifikasi pdf.pdf\")\n",
    "\n",
    "# # Simpan hasilnya\n",
    "# with open(\"output.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(md_text)\n",
    "\n",
    "# Muat dokumen Markdown\n",
    "# print(f\"Jumlah total chunk hierarkis: {len(hierarchical_chunks)}\\n\")\n",
    "\n",
    "# for i, chunk in enumerate(hierarchical_chunks):\n",
    "#     print(f\"--- Chunk #{i+1} ---\")\n",
    "#     print(f\"Metadata: {chunk.metadata}\")\n",
    "#     # Cetak 100 karakter pertama dari konten untuk pratinjau\n",
    "#     print(f\"Konten: {chunk.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae54201",
   "metadata": {},
   "source": [
    "<h5>Chunking bertipe <b>hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e7c1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_chunking_hierarchical(pdf_path: str):\n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on\n",
    "    )\n",
    "    \n",
    "    hierarchical_chunks = markdown_splitter.split_text(md_text)\n",
    "    return hierarchical_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1a2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_chunking_hierarchical(pdf_path: str):\n",
    "    import pymupdf4llm\n",
    "    import os\n",
    "    from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    # Ekstrak nama file dari path\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    # Atau tanpa ekstensi:\n",
    "    # file_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    \n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on\n",
    "    )\n",
    "\n",
    "    hierarchical_chunks = markdown_splitter.split_text(md_text)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    final_chunks = []\n",
    "    for chunk in hierarchical_chunks:\n",
    "        content = chunk.page_content\n",
    "        sub_chunks = text_splitter.split_text(content)\n",
    "\n",
    "        for sub in sub_chunks:\n",
    "            # Gabungkan metadata existing dengan nama file\n",
    "            metadata = chunk.metadata.copy()\n",
    "            metadata[\"source\"] = pdf_path  # path lengkap\n",
    "            metadata[\"file_name\"] = file_name  # nama file saja\n",
    "            \n",
    "            final_chunks.append(\n",
    "                Document(\n",
    "                    page_content=sub,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "# Saat retrieve nanti:\n",
    "# results = vector_store.similarity_search(query, k=3)\n",
    "# for doc in results:\n",
    "#     print(f\"Modul: {doc.metadata.get('file_name')}\")\n",
    "#     print(f\"Content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e692ccd",
   "metadata": {},
   "source": [
    "# <h3><b>Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c1572",
   "metadata": {},
   "source": [
    "<ol>\n",
    "<li><b>paraphrase-multilingual-MiniLM-L12-v2</b>\n",
    "<p>This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "<li><b>all-MiniLM-L6-v2</b>\n",
    "<p>This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b756cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saita\\AppData\\Local\\Temp\\ipykernel_24128\\3338366974.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings model loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    # model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "print(\"‚úÖ Embeddings model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb5753",
   "metadata": {},
   "source": [
    "# <h3><b>Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12847c2",
   "metadata": {},
   "source": [
    "## <h4><b>Store Data to Vector DB Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12c716bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_path = \"D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf\"\n",
    "pdf_path = \"D:/Codelabs/Roadmap Data Science/E-Book/Cheatsheet Statistika untuk Data Science.pdf\"\n",
    "hierarchical_chunks = pdf_chunking_hierarchical(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e23df521",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PERSIST_DIRECTORY = \"../chroma_db\"\n",
    "COLLECTION_NAME = \"machine_learning_modules\"\n",
    "\n",
    "def store_to_chromadb(hierarchical_chunks, embeddings, CHROMA_PERSIST_DIRECTORY, COLLECTION_NAME):\n",
    "    try:\n",
    "        # Load existing vector store atau buat baru\n",
    "        try:\n",
    "            vector_store = Chroma(\n",
    "                persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    "                embedding_function=embeddings,\n",
    "                collection_name=COLLECTION_NAME\n",
    "            )\n",
    "            existing_collection = True\n",
    "        except:\n",
    "            vector_store = None\n",
    "            existing_collection = False\n",
    "        \n",
    "        # Cek file_name yang akan ditambahkan\n",
    "        new_file_name = hierarchical_chunks[0].metadata.get('file_name') if hierarchical_chunks else None\n",
    "        \n",
    "        if existing_collection and new_file_name:\n",
    "            # Cek apakah file sudah ada\n",
    "            existing_docs = vector_store.get()\n",
    "            existing_file_names = set()\n",
    "            \n",
    "            if existing_docs and existing_docs.get('metadatas'):\n",
    "                for meta in existing_docs['metadatas']:\n",
    "                    if meta.get('file_name'):\n",
    "                        existing_file_names.add(meta['file_name'])\n",
    "            \n",
    "            # Jika file sudah ada, hapus data lama\n",
    "            if new_file_name in existing_file_names:\n",
    "                print(f\"‚ö†Ô∏è File '{new_file_name}' sudah ada. Menghapus data lama...\")\n",
    "                \n",
    "                # Hapus semua dokumen dengan file_name yang sama\n",
    "                ids_to_delete = []\n",
    "                for i, meta in enumerate(existing_docs['metadatas']):\n",
    "                    if meta.get('file_name') == new_file_name:\n",
    "                        ids_to_delete.append(existing_docs['ids'][i])\n",
    "                \n",
    "                if ids_to_delete:\n",
    "                    vector_store.delete(ids=ids_to_delete)\n",
    "                    print(f\"‚úÖ Berhasil menghapus {len(ids_to_delete)} dokumen lama dari '{new_file_name}'\")\n",
    "        \n",
    "        # Tambahkan dokumen baru\n",
    "        if vector_store is None:\n",
    "            vector_store = Chroma.from_documents(\n",
    "                documents=hierarchical_chunks,\n",
    "                embedding=embeddings,\n",
    "                persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    "                collection_name=COLLECTION_NAME\n",
    "            )\n",
    "        else:\n",
    "            vector_store.add_documents(hierarchical_chunks)\n",
    "        \n",
    "        # Hapus duplikat berdasarkan content dan metadata\n",
    "        print(\"üîç Memeriksa duplikat...\")\n",
    "        all_docs = vector_store.get()\n",
    "        \n",
    "        if all_docs and all_docs.get('ids'):\n",
    "            seen = set()\n",
    "            ids_to_delete = []\n",
    "            \n",
    "            for i, (doc_id, content, metadata) in enumerate(zip(\n",
    "                all_docs['ids'],\n",
    "                all_docs['documents'],\n",
    "                all_docs['metadatas']\n",
    "            )):\n",
    "                # Buat signature unik dari content + metadata penting\n",
    "                signature = (\n",
    "                    content,\n",
    "                    metadata.get('file_name'),\n",
    "                    metadata.get('Header 1'),\n",
    "                    metadata.get('Header 2'),\n",
    "                    metadata.get('Header 3')\n",
    "                )\n",
    "                \n",
    "                if signature in seen:\n",
    "                    ids_to_delete.append(doc_id)\n",
    "                else:\n",
    "                    seen.add(signature)\n",
    "            \n",
    "            # Hapus duplikat\n",
    "            if ids_to_delete:\n",
    "                vector_store.delete(ids=ids_to_delete)\n",
    "                print(f\"üóëÔ∏è Berhasil menghapus {len(ids_to_delete)} dokumen duplikat\")\n",
    "        \n",
    "        total_docs = len(vector_store.get()['ids']) if vector_store.get() else len(hierarchical_chunks)\n",
    "        \n",
    "        return f\"‚úÖ Berhasil menyimpan {len(hierarchical_chunks)} dokumen baru ke ChromaDB.\\nüìä Total dokumen di database: {total_docs}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Gagal menyimpan ke ChromaDB: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2838951e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Memeriksa duplikat...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'‚úÖ Berhasil menyimpan 151 dokumen baru ke ChromaDB.\\nüìä Total dokumen di database: 279'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_to_chromadb(hierarchical_chunks, embeddings, CHROMA_PERSIST_DIRECTORY, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764e33b",
   "metadata": {},
   "source": [
    "## <h4><b>Load Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb34d29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saita\\AppData\\Local\\Temp\\ipykernel_24128\\2346112944.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database berhasil dimuat.\n"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma(\n",
    "    persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "print(\"Database berhasil dimuat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf523eb0",
   "metadata": {},
   "source": [
    "## <h4><b>Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23045c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    # search_type=\"similarity_score_threshold\",\n",
    "    search_type=\"similarity\",\n",
    "    # search_kwargs={\"k\": 3, \"score_threshold\": 0.1}\n",
    "    search_kwargs={\"k\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de1d02",
   "metadata": {},
   "source": [
    "# <h3><b>Reranker V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfdda896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ========== CELL 8: Load Reranker Model ==========\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"üéØ LOADING RERANKER MODEL\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Load cross-encoder untuk reranking\n",
    "# reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# print(\"‚úÖ Reranker model loaded!\")\n",
    "# print(\"   ‚Ä¢ Model: cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "# print(\"   ‚Ä¢ Type: Cross-encoder (BERT-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6a1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ========== CELL 1: Enhanced Reranking Function dengan Hierarki ==========\n",
    "# def rerank_documents_hierarchical(\n",
    "#     query: str, \n",
    "#     documents: List[Document], \n",
    "#     top_n: int = 5,\n",
    "#     use_parent: bool = True,\n",
    "#     parent_boost: float = 0.1\n",
    "# ) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     Rerank documents using cross-encoder dengan support untuk hierarchical chunks\n",
    "    \n",
    "#     Args:\n",
    "#         query: Search query\n",
    "#         documents: List of retrieved documents (bisa child atau parent chunks)\n",
    "#         top_n: Number of top documents to return\n",
    "#         use_parent: Jika True, akan mencoba retrieve parent chunk jika ada\n",
    "#         parent_boost: Boost score untuk parent chunks (0.1 = +10%)\n",
    "    \n",
    "#     Returns:\n",
    "#         List of reranked documents with scores and hierarchy info\n",
    "#     \"\"\"\n",
    "#     if not documents:\n",
    "#         return []\n",
    "    \n",
    "#     print(f\"üîç Reranking {len(documents)} documents...\")\n",
    "    \n",
    "#     # Prepare pairs for cross-encoder\n",
    "#     pairs = []\n",
    "#     doc_info = []\n",
    "    \n",
    "#     for doc in documents:\n",
    "#         # Check if document has hierarchy metadata\n",
    "#         chunk_type = doc.metadata.get('chunk_type', 'unknown')\n",
    "#         has_parent = doc.metadata.get('parent_id') is not None\n",
    "        \n",
    "#         pairs.append([query, doc.page_content])\n",
    "#         doc_info.append({\n",
    "#             'doc': doc,\n",
    "#             'chunk_type': chunk_type,\n",
    "#             'has_parent': has_parent\n",
    "#         })\n",
    "    \n",
    "#     # Get scores from cross-encoder\n",
    "#     scores = reranker.predict(pairs)\n",
    "    \n",
    "#     # Apply hierarchy-aware scoring\n",
    "#     adjusted_scores = []\n",
    "#     for score, info in zip(scores, doc_info):\n",
    "#         adjusted_score = float(score)\n",
    "        \n",
    "#         # Boost parent chunks (biasanya lebih informatif)\n",
    "#         if info['chunk_type'] == 'parent':\n",
    "#             adjusted_score += parent_boost\n",
    "        \n",
    "#         # Store in metadata\n",
    "#         info['doc'].metadata['relevance_score'] = adjusted_score\n",
    "#         info['doc'].metadata['raw_score'] = float(score)\n",
    "        \n",
    "#         adjusted_scores.append(adjusted_score)\n",
    "    \n",
    "#     # Sort by adjusted score (descending)\n",
    "#     ranked_items = sorted(\n",
    "#         zip(doc_info, adjusted_scores),\n",
    "#         key=lambda x: x[1],\n",
    "#         reverse=True\n",
    "#     )\n",
    "    \n",
    "#     # Return top N\n",
    "#     top_docs = [item[0]['doc'] for item in ranked_items[:top_n]]\n",
    "    \n",
    "#     print(f\"‚úÖ Reranking complete! Top score: {ranked_items[0][1]:.4f}\")\n",
    "    \n",
    "#     return top_docs\n",
    "\n",
    "\n",
    "# # ========== CELL 2: Function to Expand to Parent Chunks ==========\n",
    "# def expand_to_parent_chunks(\n",
    "#     documents: List[Document],\n",
    "#     vector_store\n",
    "# ) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     Expand child chunks ke parent chunks jika tersedia\n",
    "    \n",
    "#     Args:\n",
    "#         documents: List of documents (might be child chunks)\n",
    "#         vector_store: ChromaDB vector store instance\n",
    "    \n",
    "#     Returns:\n",
    "#         List of documents with parent chunks included\n",
    "#     \"\"\"\n",
    "#     expanded_docs = []\n",
    "#     parent_ids_seen = set()\n",
    "    \n",
    "#     for doc in documents:\n",
    "#         # Add original document\n",
    "#         expanded_docs.append(doc)\n",
    "        \n",
    "#         # Check if has parent\n",
    "#         parent_id = doc.metadata.get('parent_id')\n",
    "        \n",
    "#         if parent_id and parent_id not in parent_ids_seen:\n",
    "#             try:\n",
    "#                 # Retrieve parent chunk by ID\n",
    "#                 parent_results = vector_store.get(ids=[parent_id])\n",
    "                \n",
    "#                 if parent_results and parent_results['documents']:\n",
    "#                     # Create Document object for parent\n",
    "#                     parent_doc = Document(\n",
    "#                         page_content=parent_results['documents'][0],\n",
    "#                         metadata=parent_results['metadatas'][0] if parent_results['metadatas'] else {}\n",
    "#                     )\n",
    "#                     expanded_docs.append(parent_doc)\n",
    "#                     parent_ids_seen.add(parent_id)\n",
    "                    \n",
    "#             except Exception as e:\n",
    "#                 print(f\"‚ö†Ô∏è Could not retrieve parent {parent_id}: {e}\")\n",
    "    \n",
    "#     return expanded_docs\n",
    "\n",
    "\n",
    "# # ========== CELL 3: Advanced Retrieval with Hierarchical Reranking ==========\n",
    "# def retrieve_and_rerank_hierarchical(\n",
    "#     query: str,\n",
    "#     vector_store,\n",
    "#     k_initial: int = 10,\n",
    "#     top_n_final: int = 5,\n",
    "#     expand_parents: bool = True,\n",
    "#     use_parent_boost: bool = True\n",
    "# ) -> Tuple[List[Document], Dict]:\n",
    "#     \"\"\"\n",
    "#     Complete retrieval pipeline dengan hierarchical support dan reranking\n",
    "    \n",
    "#     Returns:\n",
    "#         Tuple of (reranked_documents, metrics_dict)\n",
    "#     \"\"\"\n",
    "#     metrics = {}\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"üîç HIERARCHICAL RETRIEVAL + RERANKING\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     # Step 1: Initial retrieval\n",
    "#     print(f\"\\nüì• Step 1: Initial retrieval (k={k_initial})\")\n",
    "#     start = time.time()\n",
    "#     initial_results = vector_store.similarity_search(query, k=k_initial)\n",
    "#     retrieval_time = time.time() - start\n",
    "#     metrics['retrieval_time'] = retrieval_time\n",
    "#     metrics['initial_count'] = len(initial_results)\n",
    "    \n",
    "#     print(f\"   ‚úÖ Retrieved {len(initial_results)} chunks\")\n",
    "#     print(f\"   ‚è±Ô∏è  Time: {retrieval_time*1000:.2f}ms\")\n",
    "    \n",
    "#     # Analyze chunk types\n",
    "#     chunk_types = {}\n",
    "#     for doc in initial_results:\n",
    "#         chunk_type = doc.metadata.get('chunk_type', 'unknown')\n",
    "#         chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1\n",
    "    \n",
    "#     print(f\"   üìä Chunk types: {chunk_types}\")\n",
    "    \n",
    "#     # Step 2: Expand to parents (optional)\n",
    "#     if expand_parents:\n",
    "#         print(f\"\\nüîº Step 2: Expanding to parent chunks\")\n",
    "#         start = time.time()\n",
    "#         expanded_results = expand_to_parent_chunks(initial_results, vector_store)\n",
    "#         expand_time = time.time() - start\n",
    "#         metrics['expand_time'] = expand_time\n",
    "#         metrics['expanded_count'] = len(expanded_results)\n",
    "        \n",
    "#         print(f\"   ‚úÖ Expanded to {len(expanded_results)} chunks (includes parents)\")\n",
    "#         print(f\"   ‚è±Ô∏è  Time: {expand_time*1000:.2f}ms\")\n",
    "        \n",
    "#         documents_to_rerank = expanded_results\n",
    "#     else:\n",
    "#         documents_to_rerank = initial_results\n",
    "#         metrics['expand_time'] = 0\n",
    "#         metrics['expanded_count'] = len(initial_results)\n",
    "    \n",
    "#     # Step 3: Rerank\n",
    "#     print(f\"\\nüéØ Step 3: Reranking to top {top_n_final}\")\n",
    "#     start = time.time()\n",
    "#     reranked_results = rerank_documents_hierarchical(\n",
    "#         query=query,\n",
    "#         documents=documents_to_rerank,\n",
    "#         top_n=top_n_final,\n",
    "#         use_parent=True,\n",
    "#         parent_boost=0.1 if use_parent_boost else 0\n",
    "#     )\n",
    "#     rerank_time = time.time() - start\n",
    "#     metrics['rerank_time'] = rerank_time\n",
    "#     metrics['final_count'] = len(reranked_results)\n",
    "    \n",
    "#     print(f\"   ‚è±Ô∏è  Time: {rerank_time*1000:.2f}ms\")\n",
    "    \n",
    "#     # Total time\n",
    "#     total_time = retrieval_time + metrics['expand_time'] + rerank_time\n",
    "#     metrics['total_time'] = total_time\n",
    "    \n",
    "#     print(f\"\\n‚è±Ô∏è  TOTAL TIME: {total_time*1000:.2f}ms\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     return reranked_results, metrics\n",
    "\n",
    "\n",
    "# # ========== CELL 4: Display Results Function ==========\n",
    "# def display_reranked_results(documents: List[Document], query: str = None):\n",
    "#     \"\"\"\n",
    "#     Display reranked results dengan hierarchy information\n",
    "#     \"\"\"\n",
    "#     if query:\n",
    "#         print(f\"\\n‚ùì Query: '{query}'\")\n",
    "    \n",
    "#     print(f\"\\nüìö Top {len(documents)} Results:\\n\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     for i, doc in enumerate(documents, 1):\n",
    "#         # Extract metadata\n",
    "#         page = doc.metadata.get('page', 'N/A')\n",
    "#         chunk_type = doc.metadata.get('chunk_type', 'unknown')\n",
    "#         chunk_id = doc.metadata.get('chunk_id', 'N/A')\n",
    "#         parent_id = doc.metadata.get('parent_id', None)\n",
    "#         relevance_score = doc.metadata.get('relevance_score', 0)\n",
    "#         raw_score = doc.metadata.get('raw_score', 0)\n",
    "        \n",
    "#         # Header\n",
    "#         print(f\"\\n{i}. üìÑ Page {page if page == 'N/A' else page+1} | Type: {chunk_type.upper()}\")\n",
    "#         print(f\"   ID: {chunk_id}\")\n",
    "#         if parent_id:\n",
    "#             print(f\"   Parent ID: {parent_id}\")\n",
    "        \n",
    "#         # Scores\n",
    "#         print(f\"   üéØ Relevance Score: {relevance_score:.4f} (raw: {raw_score:.4f})\")\n",
    "        \n",
    "#         # Content preview\n",
    "#         content_preview = doc.page_content[:200] if len(doc.page_content) > 200 else doc.page_content\n",
    "#         print(f\"   üìù Content:\\n   {content_preview}...\")\n",
    "        \n",
    "#         # Hierarchy indicator\n",
    "#         if chunk_type == 'parent':\n",
    "#             print(f\"   üîº [PARENT CHUNK - Contains broader context]\")\n",
    "#         elif chunk_type == 'child':\n",
    "#             print(f\"   üîΩ [CHILD CHUNK - Focused content]\")\n",
    "        \n",
    "#         print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e051b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ========== CELL 5: Test Hierarchical Retrieval + Reranking ==========\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"üß™ TEST: HIERARCHICAL RETRIEVAL + RERANKING\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Test query\n",
    "# test_query = \"Apa itu overfitting?\"\n",
    "\n",
    "# # Retrieve and rerank\n",
    "# reranked_docs, metrics = retrieve_and_rerank_hierarchical(\n",
    "#     query=test_query,\n",
    "#     vector_store=vector_store,\n",
    "#     k_initial=10,           # Retrieve 10 chunks initially\n",
    "#     top_n_final=5,          # Return top 5 after reranking\n",
    "#     expand_parents=True,    # Include parent chunks\n",
    "#     use_parent_boost=True   # Boost parent chunk scores\n",
    "# )\n",
    "\n",
    "# # Display results\n",
    "# display_reranked_results(reranked_docs, query=test_query)\n",
    "\n",
    "# # Display metrics\n",
    "# print(\"\\nüìä METRICS:\")\n",
    "# print(\"=\"*70)\n",
    "# for key, value in metrics.items():\n",
    "#     if 'time' in key:\n",
    "#         print(f\"   ‚Ä¢ {key}: {value*1000:.2f}ms\")\n",
    "#     else:\n",
    "#         print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "# print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e9be8",
   "metadata": {},
   "source": [
    "# <h3><b>Reranker V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73bad8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain_community.cross_encoders import HuggingFaceCrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6687235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 4. Inisialisasi Model Reranker (LANGKAH BARU) ---\n",
    "# print(\"Memuat model reranker...\")\n",
    "# # Gunakan model cross-encoder dari BAAI yang bagus untuk multilingual\n",
    "# reranker_model = HuggingFaceCrossEncoder(\n",
    "#     model_name=\"BAAI/bge-reranker-base\",\n",
    "#     model_kwargs={'device': 'cpu'} # Ganti ke 'cuda' jika Anda memiliki GPU\n",
    "# )\n",
    "# print(\"‚úÖ Model reranker berhasil dimuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d019b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compression_retriever = ContextualCompressionRetriever(\n",
    "#     base_compressor=reranker_model, \n",
    "#     base_retriever=retriever\n",
    "# )\n",
    "# print(\"‚úÖ Compression retriever dengan reranker telah dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042b0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 6. Contoh Penggunaan ---\n",
    "# # Sekarang, gunakan `compression_retriever` untuk melakukan pencarian\n",
    "# query = \"Apa itu model klasifikasi dalam machine learning?\"\n",
    "# print(f\"\\nMelakukan pencarian dengan query: '{query}'\")\n",
    "\n",
    "# # Gunakan .invoke() untuk mendapatkan dokumen yang sudah di-rerank\n",
    "# reranked_docs = compression_retriever.invoke(query)\n",
    "\n",
    "# print(f\"\\nDitemukan {len(reranked_docs)} dokumen yang sudah di-rerank:\\n\")\n",
    "\n",
    "# # Tampilkan hasilnya\n",
    "# for i, doc in enumerate(reranked_docs):\n",
    "#     # Perhatikan bahwa metadata dari chunking hierarkis Anda masih ada\n",
    "#     print(f\"--- Dokumen #{i+1} ---\")\n",
    "#     print(f\"Metadata: {doc.metadata}\")\n",
    "#     # Tampilkan sebagian konten\n",
    "#     print(f\"Konten: {doc.page_content[:200]}...\")\n",
    "#     print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c2998a",
   "metadata": {},
   "source": [
    "# <h4><b>Delete Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a25ff944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Koleksi 'machine_learning_modules' berhasil dihapus dari database.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "try:\n",
    "    # Inisialisasi klien Chroma yang terhubung ke folder persistensi\n",
    "    client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIRECTORY)\n",
    "\n",
    "    # Hapus koleksi berdasarkan nama\n",
    "    client.delete_collection(name=COLLECTION_NAME)\n",
    "    \n",
    "    print(f\"‚úÖ Koleksi '{COLLECTION_NAME}' berhasil dihapus dari database.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gagal menghapus koleksi: {e}\")\n",
    "    print(\"Mungkin koleksi tidak ditemukan atau database belum ada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466af15",
   "metadata": {},
   "source": [
    "# <h3><b>Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b13ec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt template created\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Kamu adalah asisten AI yang membantu menjawab pertanyaan berdasarkan konteks yang diberikan.\n",
    "\n",
    "Konteks:\n",
    "{context}\n",
    "\n",
    "Pertanyaan: {question}\n",
    "\n",
    "Instruksi:\n",
    "- Jawab berdasarkan konteks yang diberikan\n",
    "- Jika tidak ada informasi di konteks, katakan \"Maaf, saya tidak menemukan informasi tersebut dalam dokumen\"\n",
    "- Berikan jawaban yang jelas dan ringkas\n",
    "- Gunakan bahasa Indonesia yang baik\n",
    "\n",
    "Jawaban:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(\"‚úÖ Prompt template created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2ddd774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt template loaded from JSON successfully\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"instructions:\n",
    "task: Tugasmu adalah menjawab pertanyaan dari mahasiswa berdasarkan dokumen modul ajar yang diberikan. Gunakan informasi dari dokumen untuk memberikan jawaban yang akurat dan relevan.\n",
    "persona: Kamu adalah seorang dosen yang menjawab pertanyaan mahasiswa dengan detail dan jelas.\n",
    "method: Untuk menjawab pertanyaan, ikuti langkah-langkah berikut:\n",
    "1. Baca pertanyaan mahasiswa dengan seksama.\n",
    "2. Cari informasi yang relevan dari dokumen modul ajar yang diberikan.\n",
    "3. Susun jawaban yang komprehensif dan mudah dipahami berdasarkan informasi tersebut.\n",
    "4. Jika informasi tidak cukup, katakan bahwa kamu tidak memiliki cukup data untuk menjawab pertanyaan tersebut.\n",
    "output-length: Jawaban harus padat sesuai dengan yang ada di dokumen.\n",
    "output-format: sebuah paragraf.\n",
    "inclusion: Penjelasan dari dokumen modul ajar yang relevan dengan pertanyaan.\n",
    "handle-unknown: Jika informasi yang diberikan tidak cukup untuk menjawab pertanyaan, katakan 'Maaf, saya tidak memiliki cukup informasi untuk menjawab pertanyaan ini.'\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"context:\n",
    "  relevant documents: \"{docs}\"\n",
    "  question: \"{query}\"\n",
    "\"\"\"\n",
    "\n",
    "# --- Buat prompt LangChain ---\n",
    "# prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", user_prompt)\n",
    "])\n",
    "print(\"‚úÖ Prompt template loaded from JSON successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2d26447",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62b8f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Daftar file yang tersimpan:\n",
      "1. 500 Pertanyaan dan Jawaban Interview Data Scientist.pdf\n",
      "2. Machine Learning _ Model Klasifikasi pdf.pdf\n",
      "3. Cheatsheet Visualisasi Data dengan Seaborn_ Teori, Penerapan, dan Code .pdf\n",
      "4. Cheatsheet Statistika untuk Data Science.pdf\n"
     ]
    }
   ],
   "source": [
    "# Ambil semua metadata\n",
    "all_data = vector_store.get()\n",
    "\n",
    "# Ambil daftar metadata dari semua dokumen\n",
    "metadatas = all_data[\"metadatas\"]\n",
    "\n",
    "# Ambil hanya field 'file_name' dan buang duplikat\n",
    "file_names = list({meta.get(\"file_name\") for meta in metadatas if meta.get(\"file_name\")})\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"üìÅ Daftar file yang tersimpan:\")\n",
    "for i, name in enumerate(file_names, 1):\n",
    "    print(f\"{i}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9521646e",
   "metadata": {},
   "source": [
    "# <h3><b>RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10af9eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG Chain created!\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"Format dokumen untuk context\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain menggunakan LCEL (LangChain Expression Language)\n",
    "rag_chain = (\n",
    "    {\"docs\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG Chain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2cfc3",
   "metadata": {},
   "source": [
    "# <h3><b>Query Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcfed12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    \"\"\"Fungsi untuk bertanya ke chatbot\"\"\"\n",
    "    print(f\"Pertanyaan: {question}\")\n",
    "    \n",
    "    # Get answer\n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    # Get source documents\n",
    "    docs = retriever.invoke(question)\n",
    "    print(\"üìö Sumber Dokumen:\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "        print(f\"Data:\\n{i}. {doc.page_content}...\")\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6c1a5",
   "metadata": {},
   "source": [
    "# <h3><b>Test Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d645ea88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melakukan pencarian untuk query: 'definisi overfitting dalam machine learning, penyebab overfitting, dampak overfitting pada akurasi model, teknik deteksi overfitting (mis‚Äëfit training vs. validation), metode pencegahan dan mitigasi overfitting (regularisasi, dropout, early stopping, cross‚Äëvalidation, pruning) serta contoh kasus overfitting pada model supervised learning.'\n"
     ]
    }
   ],
   "source": [
    "# --- 3. LAKUKAN PENCARIAN ---\n",
    "user_query = \"Apa itu overfitting?\"\n",
    "expanded_query = \"definisi overfitting dalam machine learning, penyebab overfitting, dampak overfitting pada akurasi model, teknik deteksi overfitting (mis‚Äëfit training vs. validation), metode pencegahan dan mitigasi overfitting (regularisasi, dropout, early stopping, cross‚Äëvalidation, pruning) serta contoh kasus overfitting pada model supervised learning.\"\n",
    "print(f\"\\nMelakukan pencarian untuk query: '{expanded_query}'\")\n",
    "\n",
    "relevant_docs = retriever.invoke(expanded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "787ad772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ditemukan 10 dokumen relevan:\n",
      "\n",
      "--- Dokumen #1 ---\n",
      "Metadata: {'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 1': '**Bab 1: Pendahuluan: Mengenal Dunia** **Klasifikasi dalam Machine Learning**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 3': '**1.4. Terminologi Penting dalam Klasifikasi**'}\n",
      "Filename: Machine Learning _ Model Klasifikasi pdf.pdf...\n",
      "Konten: mengidentifikasi pola dan hubungan antara fitur dan label.  \n",
      "**Data Uji (Testing Data)**  \n",
      "Data uji adalah kumpulan data yang digunakan untuk menguji kinerja model ML setelah\n",
      "proses pelatihan. Data uji terdiri dari fitur tetapi tidak menyertakan label. Model ML akan\n",
      "membuat prediksi label untuk data uji, yang kemudian dapat dibandingkan dengan label\n",
      "sebenarnya (jika diketahui) untuk menghitung metrik evaluasi.  \n",
      "7  \n",
      "**Overfitting**  \n",
      "Overfitting terjadi ketika model ML terlalu baik dalam mempelajari pola dan detail dari data\n",
      "latih, sehingga menjadi kurang mampu untuk membuat prediksi yang akurat pada data\n",
      "baru yang belum pernah dilihat sebelumnya. Overfitting biasanya terjadi ketika model ML\n",
      "terlalu kompleks dan mencoba untuk menyesuaikan diri dengan setiap titik data dalam data  \n",
      "latih.  \n",
      "**Underfitting**  \n",
      "Underfitting terjadi ketika model ML tidak mampu menangkap pola dan hubungan yang ada\n",
      "dalam data latih, sehingga menghasilkan prediksi yang buruk baik pada data latih maupun...\n",
      "\n",
      "--- Dokumen #2 ---\n",
      "Metadata: {'Header 1': '**Bab 1: Pendahuluan: Mengenal Dunia** **Klasifikasi dalam Machine Learning**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 3': '**1.4. Terminologi Penting dalam Klasifikasi**'}\n",
      "Filename: Machine Learning _ Model Klasifikasi pdf.pdf...\n",
      "Konten: dalam data latih, sehingga menghasilkan prediksi yang buruk baik pada data latih maupun\n",
      "data uji. Underfitting biasanya terjadi ketika model ML terlalu sederhana dan tidak memiliki\n",
      "kapasitas yang cukup untuk mempelajari pola yang ada dalam data latih.  \n",
      "**Regularisasi**  \n",
      "Regularisasi adalah teknik yang digunakan untuk mengurangi overfitting dalam model ML\n",
      "dengan menambahkan hukuman (penalty) terhadap kompleksitas model. Regularisasi\n",
      "mendorong model untuk menjadi lebih sederhana dan menghindari penyesuaian yang\n",
      "berlebihan terhadap data latih.  \n",
      "**Cross-Validation**  \n",
      "Cross-validation adalah teknik yang digunakan untuk mengestimasi kinerja model ML pada\n",
      "data baru yang belum pernah dilihat sebelumnya. Dalam cross-validation, data latih dibagi\n",
      "menjadi beberapa lipatan (folds), di mana model ML dilatih pada sebagian lipatan dan diuji\n",
      "pada lipatan yang tersisa. Proses ini diulangi beberapa kali dengan lipatan yang berbeda,...\n",
      "\n",
      "--- Dokumen #3 ---\n",
      "Metadata: {'Header 3': '**3.8 Optimasi KNN dengan Cross-Validation**', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 1': '**Bab 3: K-Nearest Neighbors: Algoritma** **Berbasis Jarak untuk Klasifikasi**'}\n",
      "Filename: Machine Learning _ Model Klasifikasi pdf.pdf...\n",
      "Konten: Untuk menemukan nilai k yang optimal, kita bisa menggunakan metode cross-validation.\n",
      "Berikut contoh cara melakukannya dengan menggunakan library scikit-learn:  \n",
      "17  \n",
      "Dengan menggunakan GridSearchCV, kita bisa mencari nilai k dan parameter lain yang\n",
      "optimal untuk meningkatkan performa model KNN. Selalu penting untuk melakukan tuning\n",
      "dan validasi model untuk menghindari overfitting atau underfitting....\n",
      "\n",
      "--- Dokumen #4 ---\n",
      "Metadata: {'Header 1': '**Bab 3: K-Nearest Neighbors: Algoritma** **Berbasis Jarak untuk Klasifikasi**', 'Header 3': '**3.6 Hal-Hal yang Harus Diperhatikan**', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf'}\n",
      "Filename: Machine Learning _ Model Klasifikasi pdf.pdf...\n",
      "Konten: Penting untuk menormalisasi fitur sebelum menggunakan KNN, karena fitur dengan\n",
      "rentang yang lebih besar akan memiliki pengaruh yang lebih besar pada jarak antar poin  \n",
      "data.  \n",
      "KNN sensitif terhadap jumlah tetangga (k) yang digunakan. Nilai k yang terlalu kecil bisa\n",
      "menyebabkan overfitting, sedangkan k yang terlalu besar bisa menyebabkan underfitting.\n",
      "Gunakan metode seperti cross-validation untuk menemukan nilai k yang optimal.\n",
      "KNN cenderung lambat untuk data dengan jumlah fitur yang besar, karena perhitungan\n",
      "jarak menjadi lebih kompleks....\n",
      "\n",
      "--- Dokumen #5 ---\n",
      "Metadata: {'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 1': '**Bab 1: Pendahuluan: Mengenal Dunia** **Klasifikasi dalam Machine Learning**', 'Header 3': '**1.1. Pengantar**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf'}\n",
      "Filename: Machine Learning _ Model Klasifikasi pdf.pdf...\n",
      "Konten: Machine Learning (ML) merupakan salah satu cabang ilmu dalam bidang kecerdasan\n",
      "buatan (Artificial Intelligence, AI) yang berkembang pesat dalam beberapa tahun terakhir.\n",
      "ML memungkinkan komputer untuk \"belajar\" dari data dan membuat prediksi atau\n",
      "mengambil keputusan tanpa diprogram secara eksplisit. Salah satu tugas utama dalam\n",
      "machine learning adalah klasifikasi, yang merupakan fokus utama dalam buku ini.  \n",
      "Klasifikasi adalah proses pengelompokan objek atau data ke dalam beberapa kelas atau\n",
      "kategori berdasarkan fitur-fitur yang dimilikinya. Pada bab ini, kita akan mempelajari\n",
      "konsep dasar klasifikasi dalam machine learning, jenis-jenis klasifikasi, dan beberapa\n",
      "terminologi penting yang akan sering kita temui sepanjang buku ini....\n",
      "\n",
      "--- Dokumen #6 ---\n",
      "Metadata: {'Header 2': '**Machine Learning Cheatsheet : Model** **_Klasifikasi_**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf'}\n",
      "Filename: Machine Learning _ Model Klasifikasi pdf.pdf...\n",
      "Konten: Disusun oleh Tim Datasans  \n",
      "_https://datasans.medium.com/_  \n",
      "1  \n",
      "**Table of Contents**  \n",
      "**Bab 1: Pendahuluan: Mengenal Dunia Klasifikasi dalam Machine Learning................... 6**  \n",
      "1.1. Pengantar....................................................................................................................6  \n",
      "1.2. Klasifikasi dalam Machine Learning............................................................................6  \n",
      "1.3. Jenis-jenis Klasifikasi.................................................................................................. 6  \n",
      "1.4. Terminologi Penting dalam Klasifikasi.........................................................................7  \n",
      "Fitur (Feature).............................................................................................................. 7  \n",
      "Label (Label)................................................................................................................ 7...\n",
      "\n",
      "--- Dokumen #7 ---\n",
      "Metadata: {'Header 3': '**1.4. Terminologi Penting dalam Klasifikasi**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 1': '**Bab 1: Pendahuluan: Mengenal Dunia** **Klasifikasi dalam Machine Learning**'}\n",
      "Filename: Machine Learning _ Model Klasifikasi pdf.pdf...\n",
      "Konten: Berikut adalah beberapa istilah penting yang perlu kita ketahui dalam klasifikasi machine\n",
      "learning:  \n",
      "**Fitur (Feature)**  \n",
      "Fitur adalah variabel input yang digunakan untuk menggambarkan objek atau data yang\n",
      "akan diklasifikasikan. Fitur dapat berupa numerik, kategorikal, atau bahkan teks. Sebagai\n",
      "contoh, dalam diagnostik medis, fitur bisa meliputi usia, jenis kelamin, dan hasil tes darah.  \n",
      "**Label (Label)**  \n",
      "Label adalah variabel target yang menjadi output dalam klasifikasi. Label menggambarkan\n",
      "kelas atau kategori yang sebenarnya dari objek atau data. Misalnya, dalam deteksi spam,\n",
      "label bisa berupa \"spam\" atau \"bukan spam.\"  \n",
      "**Data Latih (Training Data)**  \n",
      "Data latih adalah kumpulan data yang digunakan untuk melatih model ML. Data latih terdiri\n",
      "dari fitur dan label yang sudah diketahui. Model ML akan \"belajar\" dari data latih untuk\n",
      "mengidentifikasi pola dan hubungan antara fitur dan label.  \n",
      "**Data Uji (Testing Data)**...\n",
      "\n",
      "--- Dokumen #8 ---\n",
      "Metadata: {'Header 1': '**Bab 11: Praktik Terbaik dan Penerapan** **Model Klasifikasi dalam Dunia Nyata**', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 3': '**11.2 Memilih Model Klasifikasi yang Tepat**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf'}\n",
      "Filename: Machine Learning _ Model Klasifikasi pdf.pdf...\n",
      "Konten: Sebelum kita membahas contoh kasus nyata, mari kita bahas cara memilih model klasifikasi\n",
      "yang paling sesuai untuk suatu masalah. Beberapa faktor yang perlu diperhatikan saat\n",
      "memilih model klasifikasi antara lain:  \n",
      "Kompleksitas data: Jika data sangat kompleks atau memiliki banyak fitur, model yang lebih\n",
      "fleksibel seperti Neural Networks atau Random Forest mungkin lebih sesuai.  \n",
      "Ukuran data: Untuk dataset yang sangat besar, model yang lebih sederhana seperti Regresi\n",
      "Logistik atau Naive Bayes mungkin lebih efisien dari segi waktu pelatihan dan memori.  \n",
      "Keseimbangan kelas: Jika kelas dalam data sangat tidak seimbang, penting untuk memilih\n",
      "model yang bisa menangani ketidakseimbangan ini, seperti metode Ensemble dengan\n",
      "teknik resampling atau model yang menggunakan fungsi biaya yang sensitif terhadap kelas.  \n",
      "Interpretasi model: Jika interpretasi model sangat penting, model yang lebih mudah\n",
      "diinterpretasikan seperti Decision Trees atau Regresi Logistik mungkin lebih sesuai....\n",
      "\n",
      "--- Dokumen #9 ---\n",
      "Metadata: {'Header 1': '**Bab 3: K-Nearest Neighbors: Algoritma** **Berbasis Jarak untuk Klasifikasi**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 3': '**3.10 Kesimpulan**', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf'}\n",
      "Filename: Machine Learning _ Model Klasifikasi pdf.pdf...\n",
      "Konten: KNN merupakan algoritma yang mudah dipahami dan diimplementasikan. Cocok untuk\n",
      "masalah klasifikasi dengan data non-linear dan jumlah fitur yang kecil. Namun, KNN  \n",
      "18  \n",
      "memiliki beberapa kelemahan, seperti sensitivitas terhadap jumlah tetangga, rentan\n",
      "terhadap data berisik, dan waktu komputasi yang lama untuk dataset besar.  \n",
      "Untuk meningkatkan performa model KNN, penting untuk menormalisasi fitur,\n",
      "menemukan nilai k yang optimal, serta melakukan tuning parameter menggunakan metode\n",
      "seperti cross-validation. Dalam beberapa kasus, menggabungkan KNN dengan metode\n",
      "ensemble seperti bagging atau boosting bisa meningkatkan akurasi dan keandalan model.  \n",
      "19...\n",
      "\n",
      "--- Dokumen #10 ---\n",
      "Metadata: {'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 1': '**Bab 10: Evaluasi Model Klasifikasi:** **Metrik dan Cara** **Menginterpretasikannya**', 'Header 3': '**10.1 Pendahuluan**', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf'}\n",
      "Filename: Machine Learning _ Model Klasifikasi pdf.pdf...\n",
      "Konten: Evaluasi model klasifikasi adalah langkah penting dalam proses pengembangan dan\n",
      "penilaian model machine learning. Pemilihan metrik yang tepat untuk mengukur kinerja\n",
      "model sangat penting untuk memastikan model yang akurat, relevan, dan bisa diandalkan....\n"
     ]
    }
   ],
   "source": [
    "# --- 4. TAMPILKAN HASIL ---\n",
    "if relevant_docs:\n",
    "    print(f\"\\nDitemukan {len(relevant_docs)} dokumen relevan:\")\n",
    "    for i, doc in enumerate(relevant_docs):\n",
    "        print(f\"\\n--- Dokumen #{i+1} ---\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "        print(f\"Filename: {doc.metadata['file_name']}...\")\n",
    "        print(f\"Konten: {doc.page_content}...\")\n",
    "else:\n",
    "    print(\"Tidak ada dokumen relevan ditemukan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3c8eaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pass_docs = retriever.get_relevant_documents(\"Bagaimana mengatasi overfitting dalam machine learning?\")\n",
    "\n",
    "selector_prompt = \"\"\"\n",
    "Dari dokumen berikut, pilih yang paling menjawab pertanyaan pengguna: \"{query}\".\n",
    "Berikan hanya teks yang paling relevan.\n",
    "docs: \"{docs}\"\n",
    "\"\"\"\n",
    "selected_docs = llm.invoke(selector_prompt.format(query=\"Bagaimana mengatasi overfitting dalam machine learning?\", docs=first_pass_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "edbdac8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': '**Bab 1: Pendahuluan: Mengenal Dunia** **Klasifikasi dalam Machine Learning**', 'Header 3': '**1.4. Terminologi Penting dalam Klasifikasi**'}, page_content='mengidentifikasi pola dan hubungan antara fitur dan label.  \\n**Data Uji (Testing Data)**  \\nData uji adalah kumpulan data yang digunakan untuk menguji kinerja model ML setelah\\nproses pelatihan. Data uji terdiri dari fitur tetapi tidak menyertakan label. Model ML akan\\nmembuat prediksi label untuk data uji, yang kemudian dapat dibandingkan dengan label\\nsebenarnya (jika diketahui) untuk menghitung metrik evaluasi.  \\n7  \\n**Overfitting**  \\nOverfitting terjadi ketika model ML terlalu baik dalam mempelajari pola dan detail dari data\\nlatih, sehingga menjadi kurang mampu untuk membuat prediksi yang akurat pada data\\nbaru yang belum pernah dilihat sebelumnya. Overfitting biasanya terjadi ketika model ML\\nterlalu kompleks dan mencoba untuk menyesuaikan diri dengan setiap titik data dalam data  \\nlatih.  \\n**Underfitting**  \\nUnderfitting terjadi ketika model ML tidak mampu menangkap pola dan hubungan yang ada\\ndalam data latih, sehingga menghasilkan prediksi yang buruk baik pada data latih maupun'),\n",
       " Document(metadata={'Header 1': '**Bab 9: Ensemble Methods:** **Meningkatkan Performa dengan** **Metode Gabungan**', 'Header 3': '**9.7 Hal-hal yang Harus Diperhatikan**'}, page_content='- Pilih model dasar yang beragam untuk mengurangi bias dan varians  \\n- Jangan menggunakan terlalu banyak model dasar yang kompleks untuk menghindari\\noverfitting  \\n- Pertimbangkan keseimbangan antara akurasi dan kompleksitas model dalam ensemble'),\n",
       " Document(metadata={'Header 2': '**Machine Learning Cheatsheet : Model** **_Klasifikasi_**'}, page_content='Disusun oleh Tim Datasans  \\n_https://datasans.medium.com/_  \\n1  \\n**Table of Contents**  \\n**Bab 1: Pendahuluan: Mengenal Dunia Klasifikasi dalam Machine Learning................... 6**  \\n1.1. Pengantar....................................................................................................................6  \\n1.2. Klasifikasi dalam Machine Learning............................................................................6  \\n1.3. Jenis-jenis Klasifikasi.................................................................................................. 6  \\n1.4. Terminologi Penting dalam Klasifikasi.........................................................................7  \\nFitur (Feature).............................................................................................................. 7  \\nLabel (Label)................................................................................................................ 7'),\n",
       " Document(metadata={'Header 1': '**Bab 9: Ensemble Methods:** **Meningkatkan Performa dengan** **Metode Gabungan**', 'Header 3': '**9.5 Kasus-kasus yang Direkomendasikan**'}, page_content='Ensemble methods direkomendasikan dalam situasi berikut:  \\n40  \\nKetika model dasar mengalami overfitting atau underfitting\\nKetika model dasar memiliki kecenderungan yang berbeda dan varians\\nKetika menghadapi masalah klasifikasi yang kompleks atau data yang sangat tidak seimbang'),\n",
       " Document(metadata={'Header 3': '**1.4. Terminologi Penting dalam Klasifikasi**', 'Header 1': '**Bab 1: Pendahuluan: Mengenal Dunia** **Klasifikasi dalam Machine Learning**'}, page_content='dalam data latih, sehingga menghasilkan prediksi yang buruk baik pada data latih maupun\\ndata uji. Underfitting biasanya terjadi ketika model ML terlalu sederhana dan tidak memiliki\\nkapasitas yang cukup untuk mempelajari pola yang ada dalam data latih.  \\n**Regularisasi**  \\nRegularisasi adalah teknik yang digunakan untuk mengurangi overfitting dalam model ML\\ndengan menambahkan hukuman (penalty) terhadap kompleksitas model. Regularisasi\\nmendorong model untuk menjadi lebih sederhana dan menghindari penyesuaian yang\\nberlebihan terhadap data latih.  \\n**Cross-Validation**  \\nCross-validation adalah teknik yang digunakan untuk mengestimasi kinerja model ML pada\\ndata baru yang belum pernah dilihat sebelumnya. Dalam cross-validation, data latih dibagi\\nmenjadi beberapa lipatan (folds), di mana model ML dilatih pada sebagian lipatan dan diuji\\npada lipatan yang tersisa. Proses ini diulangi beberapa kali dengan lipatan yang berbeda,'),\n",
       " Document(metadata={'Header 1': '**Bab 1: Pendahuluan: Mengenal Dunia** **Klasifikasi dalam Machine Learning**', 'Header 3': '**1.1. Pengantar**'}, page_content='Machine Learning (ML) merupakan salah satu cabang ilmu dalam bidang kecerdasan\\nbuatan (Artificial Intelligence, AI) yang berkembang pesat dalam beberapa tahun terakhir.\\nML memungkinkan komputer untuk \"belajar\" dari data dan membuat prediksi atau\\nmengambil keputusan tanpa diprogram secara eksplisit. Salah satu tugas utama dalam\\nmachine learning adalah klasifikasi, yang merupakan fokus utama dalam buku ini.  \\nKlasifikasi adalah proses pengelompokan objek atau data ke dalam beberapa kelas atau\\nkategori berdasarkan fitur-fitur yang dimilikinya. Pada bab ini, kita akan mempelajari\\nkonsep dasar klasifikasi dalam machine learning, jenis-jenis klasifikasi, dan beberapa\\nterminologi penting yang akan sering kita temui sepanjang buku ini.'),\n",
       " Document(metadata={'Header 3': '**11.2 Memilih Model Klasifikasi yang Tepat**', 'Header 1': '**Bab 11: Praktik Terbaik dan Penerapan** **Model Klasifikasi dalam Dunia Nyata**'}, page_content='Sebelum kita membahas contoh kasus nyata, mari kita bahas cara memilih model klasifikasi\\nyang paling sesuai untuk suatu masalah. Beberapa faktor yang perlu diperhatikan saat\\nmemilih model klasifikasi antara lain:  \\nKompleksitas data: Jika data sangat kompleks atau memiliki banyak fitur, model yang lebih\\nfleksibel seperti Neural Networks atau Random Forest mungkin lebih sesuai.  \\nUkuran data: Untuk dataset yang sangat besar, model yang lebih sederhana seperti Regresi\\nLogistik atau Naive Bayes mungkin lebih efisien dari segi waktu pelatihan dan memori.  \\nKeseimbangan kelas: Jika kelas dalam data sangat tidak seimbang, penting untuk memilih\\nmodel yang bisa menangani ketidakseimbangan ini, seperti metode Ensemble dengan\\nteknik resampling atau model yang menggunakan fungsi biaya yang sensitif terhadap kelas.  \\nInterpretasi model: Jika interpretasi model sangat penting, model yang lebih mudah\\ndiinterpretasikan seperti Decision Trees atau Regresi Logistik mungkin lebih sesuai.'),\n",
       " Document(metadata={'Header 3': '**1.4. Terminologi Penting dalam Klasifikasi**', 'Header 1': '**Bab 1: Pendahuluan: Mengenal Dunia** **Klasifikasi dalam Machine Learning**'}, page_content='pada lipatan yang tersisa. Proses ini diulangi beberapa kali dengan lipatan yang berbeda,\\ndan hasilnya dirata-ratakan untuk menghasilkan estimasi kinerja yang lebih stabil dan\\ndapat diandalkan.  \\n**Rangkuman**  \\nDalam bab-bab berikutnya, kita akan mempelajari berbagai algoritma klasifikasi yang\\npopuler, mulai dari regresi logistik hingga metode gabungan (ensemble methods), serta cara\\nmengevaluasi dan menginterpretasi kinerja model klasifikasi. Tujuan utama buku ini\\nadalah untuk membantu Anda memahami konsep dasar dan teknik-teknik klasifikasi dalam\\nmachine learning, sehingga Anda dapat mengaplikasikannya dalam berbagai masalah dunia  \\nnyata.  \\n8'),\n",
       " Document(metadata={'Header 1': '**Bab 3: K-Nearest Neighbors: Algoritma** **Berbasis Jarak untuk Klasifikasi**', 'Header 3': '**3.8 Optimasi KNN dengan Cross-Validation**'}, page_content='Untuk menemukan nilai k yang optimal, kita bisa menggunakan metode cross-validation.\\nBerikut contoh cara melakukannya dengan menggunakan library scikit-learn:  \\n17  \\nDengan menggunakan GridSearchCV, kita bisa mencari nilai k dan parameter lain yang\\noptimal untuk meningkatkan performa model KNN. Selalu penting untuk melakukan tuning\\ndan validasi model untuk menghindari overfitting atau underfitting.'),\n",
       " Document(metadata={'Header 3': '**1.3. Jenis-jenis Klasifikasi**', 'Header 1': '**Bab 1: Pendahuluan: Mengenal Dunia** **Klasifikasi dalam Machine Learning**'}, page_content='Klasifikasi dalam machine learning umumnya dibagi menjadi dua jenis, yaitu:  \\n6  \\nKlasifikasi Binomial (Binary Classification): Klasifikasi ini hanya melibatkan dua kelas atau\\nkategori. Contohnya adalah deteksi spam yang membedakan antara email spam dan bukan  \\nspam.  \\nKlasifikasi Multiclass (Multiclass Classification): Klasifikasi ini melibatkan lebih dari dua\\nkelas atau kategori. Contohnya adalah pengenalan karakter tulisan tangan yang melibatkan\\nbanyak kelas, yaitu angka 0-9 dan huruf A-Z.')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_pass_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa4d31bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Regularisasi**  \\nRegularisasi adalah teknik yang digunakan untuk mengurangi overfitting dalam model ML dengan menambahkan hukuman (penalty) terhadap kompleksitas model. Regularisasi mendorong model untuk menjadi lebih sederhana dan menghindari penyesuaian yang berlebihan terhadap data latih.  \\n\\n**Cross‚ÄëValidation**  \\nCross‚Äëvalidation adalah teknik yang digunakan untuk mengestimasi kinerja model ML pada data baru yang belum pernah dilihat sebelumnya. Dalam cross‚Äëvalidation, data latih dibagi menjadi beberapa lipatan (folds), di mana model ML dilatih pada sebagian lipatan dan diuji pada lipatan yang tersisa. Proses ini diulangi beberapa kali dengan lipatan yang berbeda.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_docs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bee5819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Query yang direformulasi:**\n",
      "\n",
      "> ‚Äúdefinisi overfitting dalam machine learning, penyebab overfitting, dampak overfitting pada akurasi model, teknik deteksi overfitting (mis‚Äëfit training vs. validation), metode pencegahan dan mitigasi overfitting (regularisasi, dropout, early stopping, cross‚Äëvalidation, pruning) serta contoh kasus overfitting pada model supervised learning.‚Äù\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt_query = PromptTemplate.from_template(\"\"\"\n",
    "Kamu adalah asisten pencarian. Reformulasikan query berikut agar lebih cocok untuk pencarian dokumen teknis machine learning.\n",
    "\n",
    "Query asli: \"{query}\"\n",
    "Kata Kunci yang  Dicari:\n",
    "\n",
    "Berikan versi yang lebih spesifik dan kaya kata kunci, tanpa mengubah makna.\n",
    "\"\"\")\n",
    "\n",
    "query_rewriter = LLMChain(llm=llm, prompt=prompt_query)\n",
    "\n",
    "user_query = \"apa itu overfitting\"\n",
    "expanded_query = query_rewriter.run(query=user_query)\n",
    "\n",
    "print(expanded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba2caa",
   "metadata": {},
   "source": [
    "# <h3><b>Test Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5184a527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pertanyaan: Apa itu decision tree dan bagaimana cara kerjanya?\n",
      "üìö Sumber Dokumen:\n",
      "Metadata: {'Header 1': '**Bab 5: Decision Trees: Klasifikasi** **Berbasis Struktur Pohon Keputusan**', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 3': '**5.4 Kasus yang Direkomendasikan untuk Menggunakan** **Decision Trees**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf'}\n",
      "Data:\n",
      "1. Decision Trees sangat cocok untuk:  \n",
      "1. Klasifikasi data yang memiliki fitur kategorikal atau numerik\n",
      "2. Masalah yang memerlukan interpretasi yang mudah dan bisa dijelaskan\n",
      "3. Ketika overfitting bisa dikendalikan dengan membatasi kedalaman pohon atau\n",
      "prunning...\n",
      "Metadata: {'Header 3': '**5.1 Pendahuluan**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 1': '**Bab 5: Decision Trees: Klasifikasi** **Berbasis Struktur Pohon Keputusan**'}\n",
      "Data:\n",
      "2. Decision Trees (Pohon Keputusan) merupakan metode klasifikasi yang populer dan mudah\n",
      "diinterpretasi. Mereka membangun struktur pohon yang berdasarkan pada fitur dan\n",
      "kondisi yang dipilih untuk memecah dataset menjadi kelas yang berbeda....\n",
      "Metadata: {'Header 2': '**Machine Learning Cheatsheet : Model** **_Klasifikasi_**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf'}\n",
      "Data:\n",
      "3. 5.8 Kelebihan dan Kekurangan Decision Trees............................................................... 26  \n",
      "**Bab 6: Random Forest: Menggabungkan Kekuatan Decision Trees.............................. 27**  \n",
      "6.1 Pendahuluan.............................................................................................................. 27  \n",
      "6.2 Algoritma Random Forest.......................................................................................... 27  \n",
      "6.3 Matematika di Belakang Random Forest................................................................... 27  \n",
      "6.4 Kasus yang Direkomendasikan untuk Menggunakan Random Forest...................... 27  \n",
      "6.5 Contoh Kasus di Dunia Nyata dan Python Script.......................................................27  \n",
      "6.6 Penjelasan Parameter untuk Hyperparameter Tuning............................................... 28...\n",
      "Metadata: {'Header 2': '**Machine Learning Cheatsheet : Model** **_Klasifikasi_**', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf'}\n",
      "Data:\n",
      "4. **Bab 5: Decision Trees: Klasifikasi Berbasis Struktur Pohon Keputusan.......................24**  \n",
      "5.1 Pendahuluan.............................................................................................................. 24  \n",
      "5.2 Algoritma Decision Trees........................................................................................... 24  \n",
      "5.3 Matematika di Belakang Decision Trees.................................................................... 24  \n",
      "5.4 Kasus yang Direkomendasikan untuk Menggunakan Decision Trees....................... 24  \n",
      "5.5 Contoh Kasus di Dunia Nyata dan Python Script.......................................................24  \n",
      "5.6 Penjelasan Parameter untuk Hyperparameter Tuning............................................... 25  \n",
      "5.7 Hal-Hal yang Harus Diperhatikan...............................................................................25...\n",
      "Metadata: {'Header 1': '**Bab 6: Random Forest:** **Menggabungkan Kekuatan Decision** **Trees**', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 3': '**6.2 Algoritma Random Forest**'}\n",
      "Data:\n",
      "5. Random Forest bekerja dengan membuat sejumlah Decision Trees dan menggabungkan\n",
      "hasil prediksi mereka melalui metode voting. Setiap pohon dibangun dengan menggunakan\n",
      "bagian acak dari data latih dan fitur, proses ini dikenal sebagai 'bagging' dan 'feature\n",
      "randomness'....\n",
      "Metadata: {'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 1': '**Bab 6: Random Forest:** **Menggabungkan Kekuatan Decision** **Trees**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 3': '**6.3 Matematika di Belakang Random Forest**'}\n",
      "Data:\n",
      "6. Random Forest tidak memiliki matematika yang spesifik di luar Decision Trees, karena\n",
      "prinsip dasarnya adalah menggabungkan kekuatan beberapa pohon keputusan....\n",
      "Metadata: {'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 3': '**6.8 Kelebihan dan Kekurangan Random Forest**', 'Header 1': '**Bab 6: Random Forest:** **Menggabungkan Kekuatan Decision** **Trees**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf'}\n",
      "Data:\n",
      "7. |Kelebihan|Kekurangan|\n",
      "|---|---|\n",
      "|Mengurangi overftting|Memerlukan lebih banyak waktu pelatihan|\n",
      "|bisa menangani ftur kategorikal dan numerik|Memerlukan lebih banyak memori|\n",
      "|Memberikan pentingnya ftur|Sulit diinterpretasi dibandingkan dengan Decision<br>Trees tunggal|\n",
      "|Tidak memerlukan normalisasi data|Tidak memerlukan normalisasi data|  \n",
      "29  \n",
      "30  \n",
      "31...\n",
      "Metadata: {'Header 3': '**6.1 Pendahuluan**', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 1': '**Bab 6: Random Forest:** **Menggabungkan Kekuatan Decision** **Trees**'}\n",
      "Data:\n",
      "8. Random Forest adalah metode klasifikasi yang menggabungkan kekuatan beberapa\n",
      "Decision Trees untuk menghasilkan prediksi yang lebih baik. Dalam proses ini, algoritma\n",
      "mengurangi varians dan overfitting yang mungkin terjadi pada Decision Trees tunggal....\n",
      "Metadata: {'Header 1': '**Bab 8: Neural Networks - Klasifikasi** **dengan Menggunakan Jaringan Saraf** **Tiruan**', 'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 3': '**8.4 Kasus Penggunaan yang Direkomendasikan**'}\n",
      "Data:\n",
      "9. JST cocok untuk digunakan pada kasus-kasus berikut:  \n",
      "1. Klasifikasi gambar dan pengenalan pola\n",
      "2. Analisis teks dan pemrosesan bahasa alami  \n",
      "36  \n",
      "3. Sistem rekomendasi\n",
      "4. Klasifikasi data berskala besar dengan banyak fitur dan non-linearitas...\n",
      "Metadata: {'source': 'D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf', 'file_name': 'Machine Learning _ Model Klasifikasi pdf.pdf', 'Header 3': '**11.3 Contoh Kasus Nyata**', 'Header 1': '**Bab 11: Praktik Terbaik dan Penerapan** **Model Klasifikasi dalam Dunia Nyata**'}\n",
      "Data:\n",
      "10. Berikut adalah tiga contoh kasus nyata yang menunjukkan bagaimana model klasifikasi bisa\n",
      "diterapkan dalam berbagai situasi menggunakan data asli yang diambil dari internet.  \n",
      "52...\n",
      "Jawaban: Decision Tree (Pohon Keputusan) adalah metode klasifikasi yang populer dan mudah diinterpretasi; ia membangun struktur pohon dengan memecah dataset menjadi cabang‚Äëcabang berdasarkan fitur dan kondisi yang dipilih secara berurutan, sehingga setiap daun pohon mewakili sebuah kelas. Cara kerjanya dimulai dengan memilih fitur yang paling informatif (misalnya menggunakan ukuran impuritas seperti Gini atau Entropy) untuk membagi data pada node akar, kemudian proses pemilihan fitur dan pemisahan berulang dilakukan secara rekursif pada setiap node hingga mencapai kriteria penghentian seperti kedalaman maksimum, jumlah sampel minimum, atau tidak ada lagi peningkatan impuritas. Hasil akhir adalah sebuah pohon keputusan yang dapat digunakan untuk mengklasifikasikan data baru dengan menelusuri cabang‚Äëcabang sesuai nilai fitur hingga mencapai daun yang memberikan prediksi kelas.\n"
     ]
    }
   ],
   "source": [
    "jawaban = ask_question(\"Apa itu decision tree dan bagaimana cara kerjanya?\")\n",
    "print(f\"Jawaban: {jawaban}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdadcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚ùì Pertanyaan: Siapa presiden Indonesia?\n",
      "============================================================\n",
      "\n",
      "üí¨ Jawaban:\n",
      "Maaf, saya tidak menemukan informasi tersebut dalam dokumen.\n",
      "\n",
      "============================================================\n",
      "üìö Sumber Dokumen:\n",
      "\n",
      "1. Akurasi adalah metrik yang paling sederhana dan intuitif. Akurasi menggambarkan\n",
      "seberapa sering model membuat prediksi yang benar.  \n",
      "Akurasi = (Jumlah...\n",
      "\n",
      "2. Contoh: Mengklasifikasikan jenis kelamin berdasarkan tinggi, berat, dan ukuran sepatu.  \n",
      "24...\n",
      "\n",
      "3. JST cocok untuk digunakan pada kasus-kasus berikut:  \n",
      "1. Klasifikasi gambar dan pengenalan pola\n",
      "2. Analisis teks dan pemrosesan bahasa alami  \n",
      "36  \n",
      "3....\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Maaf, saya tidak menemukan informasi tersebut dalam dokumen.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Siapa presiden Indonesia?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
