{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc27432",
   "metadata": {},
   "source": [
    "# <h3><b>Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be3358c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "import pymupdf4llm\n",
    "import glob\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import CrossEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b7ad4",
   "metadata": {},
   "source": [
    "# <h3><b>Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d72bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8b8fd",
   "metadata": {},
   "source": [
    "# <h3><b>LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c590ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM initialized!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=KEY,\n",
    "    model_name=\"openai/gpt-oss-120b\",\n",
    "    temperature=0\n",
    ")\n",
    "print(\"✅ LLM initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0353c8",
   "metadata": {},
   "source": [
    "# <h3><b>Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38bf356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ekstrak PDF ke Markdown\n",
    "# md_text = pymupdf4llm.to_markdown(r\"D:\\Portfolio\\rag-llm-education\\data\\documents\\Machine Learning _ Model Klasifikasi pdf.pdf\")\n",
    "\n",
    "# # Simpan hasilnya\n",
    "# with open(\"output.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(md_text)\n",
    "\n",
    "# Muat dokumen Markdown\n",
    "# print(f\"Jumlah total chunk hierarkis: {len(hierarchical_chunks)}\\n\")\n",
    "\n",
    "# for i, chunk in enumerate(hierarchical_chunks):\n",
    "#     print(f\"--- Chunk #{i+1} ---\")\n",
    "#     print(f\"Metadata: {chunk.metadata}\")\n",
    "#     # Cetak 100 karakter pertama dari konten untuk pratinjau\n",
    "#     print(f\"Konten: {chunk.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7c1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pdf_chunking_hierarchical(pdf_path: str):\n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on\n",
    "    )\n",
    "    \n",
    "    hierarchical_chunks = markdown_splitter.split_text(md_text)\n",
    "    return hierarchical_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e692ccd",
   "metadata": {},
   "source": [
    "# <h3><b>Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a64dd6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (0.5.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saita\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages (from requests->transformers) (2025.10.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\users\\\\saita\\\\anaconda3\\\\envs\\\\portfolio\\\\lib\\\\site-packages\\\\charset_normalizer-3.4.3.dist-info\\\\METADATA'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers transformers torch ml-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b756cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saita\\AppData\\Local\\Temp\\ipykernel_26040\\3410920291.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings model loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    # model_name=\"Qwen/Qwen3-Embedding-0.6B\"\n",
    ")\n",
    "print(\"✅ Embeddings model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb5753",
   "metadata": {},
   "source": [
    "# <h3><b>Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b9ecab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=\"../chroma_db\"\n",
    "# )\n",
    "# print(\"✅ Vector store created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c107d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever(\n",
    "#     search_type=\"similarity\",\n",
    "#     search_kwargs={\"k\": 3}\n",
    "# )\n",
    "# print(\"✅ Retriever ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e23df521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil menyimpan 104 dokumen ke database Chroma di direktori '../chroma_db'\n"
     ]
    }
   ],
   "source": [
    "hierarchical_chunks = pdf_chunking_hierarchical(pdf_path=\"D:/Portfolio/rag-llm-education/data/documents/Machine Learning _ Model Klasifikasi pdf.pdf\")\n",
    "\n",
    "CHROMA_PERSIST_DIRECTORY = \"../chroma_db\"\n",
    "COLLECTION_NAME = \"machine_learning_modules\"\n",
    "\n",
    "try:\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=hierarchical_chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    "        collection_name=COLLECTION_NAME\n",
    "    )\n",
    "\n",
    "    print(f\"Berhasil menyimpan {len(hierarchical_chunks)} dokumen ke database Chroma di direktori '{CHROMA_PERSIST_DIRECTORY}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Gagal menyimpan ke ChromaDB: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb34d29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database berhasil dimuat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saita\\AppData\\Local\\Temp\\ipykernel_26040\\2346112944.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma(\n",
    "    persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "print(\"Database berhasil dimuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23045c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    # search_type=\"similarity_score_threshold\",\n",
    "    search_type=\"similarity\",\n",
    "    # search_kwargs={\"k\": 3, \"score_threshold\": 0.1}\n",
    "    search_kwargs={\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d189ed65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de1d02",
   "metadata": {},
   "source": [
    "# <h3><b>Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfdda896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🎯 LOADING RERANKER MODEL\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saita\\anaconda3\\envs\\portfolio\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saita\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reranker model loaded!\n",
      "   • Model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "   • Type: Cross-encoder (BERT-based)\n"
     ]
    }
   ],
   "source": [
    "# ========== CELL 8: Load Reranker Model ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎯 LOADING RERANKER MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load cross-encoder untuk reranking\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "print(\"✅ Reranker model loaded!\")\n",
    "print(\"   • Model: cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"   • Type: Cross-encoder (BERT-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b6a1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 1: Enhanced Reranking Function dengan Hierarki ==========\n",
    "def rerank_documents_hierarchical(\n",
    "    query: str, \n",
    "    documents: List[Document], \n",
    "    top_n: int = 5,\n",
    "    use_parent: bool = True,\n",
    "    parent_boost: float = 0.1\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Rerank documents using cross-encoder dengan support untuk hierarchical chunks\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        documents: List of retrieved documents (bisa child atau parent chunks)\n",
    "        top_n: Number of top documents to return\n",
    "        use_parent: Jika True, akan mencoba retrieve parent chunk jika ada\n",
    "        parent_boost: Boost score untuk parent chunks (0.1 = +10%)\n",
    "    \n",
    "    Returns:\n",
    "        List of reranked documents with scores and hierarchy info\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return []\n",
    "    \n",
    "    print(f\"🔍 Reranking {len(documents)} documents...\")\n",
    "    \n",
    "    # Prepare pairs for cross-encoder\n",
    "    pairs = []\n",
    "    doc_info = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Check if document has hierarchy metadata\n",
    "        chunk_type = doc.metadata.get('chunk_type', 'unknown')\n",
    "        has_parent = doc.metadata.get('parent_id') is not None\n",
    "        \n",
    "        pairs.append([query, doc.page_content])\n",
    "        doc_info.append({\n",
    "            'doc': doc,\n",
    "            'chunk_type': chunk_type,\n",
    "            'has_parent': has_parent\n",
    "        })\n",
    "    \n",
    "    # Get scores from cross-encoder\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Apply hierarchy-aware scoring\n",
    "    adjusted_scores = []\n",
    "    for score, info in zip(scores, doc_info):\n",
    "        adjusted_score = float(score)\n",
    "        \n",
    "        # Boost parent chunks (biasanya lebih informatif)\n",
    "        if info['chunk_type'] == 'parent':\n",
    "            adjusted_score += parent_boost\n",
    "        \n",
    "        # Store in metadata\n",
    "        info['doc'].metadata['relevance_score'] = adjusted_score\n",
    "        info['doc'].metadata['raw_score'] = float(score)\n",
    "        \n",
    "        adjusted_scores.append(adjusted_score)\n",
    "    \n",
    "    # Sort by adjusted score (descending)\n",
    "    ranked_items = sorted(\n",
    "        zip(doc_info, adjusted_scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Return top N\n",
    "    top_docs = [item[0]['doc'] for item in ranked_items[:top_n]]\n",
    "    \n",
    "    print(f\"✅ Reranking complete! Top score: {ranked_items[0][1]:.4f}\")\n",
    "    \n",
    "    return top_docs\n",
    "\n",
    "\n",
    "# ========== CELL 2: Function to Expand to Parent Chunks ==========\n",
    "def expand_to_parent_chunks(\n",
    "    documents: List[Document],\n",
    "    vector_store\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Expand child chunks ke parent chunks jika tersedia\n",
    "    \n",
    "    Args:\n",
    "        documents: List of documents (might be child chunks)\n",
    "        vector_store: ChromaDB vector store instance\n",
    "    \n",
    "    Returns:\n",
    "        List of documents with parent chunks included\n",
    "    \"\"\"\n",
    "    expanded_docs = []\n",
    "    parent_ids_seen = set()\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Add original document\n",
    "        expanded_docs.append(doc)\n",
    "        \n",
    "        # Check if has parent\n",
    "        parent_id = doc.metadata.get('parent_id')\n",
    "        \n",
    "        if parent_id and parent_id not in parent_ids_seen:\n",
    "            try:\n",
    "                # Retrieve parent chunk by ID\n",
    "                parent_results = vector_store.get(ids=[parent_id])\n",
    "                \n",
    "                if parent_results and parent_results['documents']:\n",
    "                    # Create Document object for parent\n",
    "                    parent_doc = Document(\n",
    "                        page_content=parent_results['documents'][0],\n",
    "                        metadata=parent_results['metadatas'][0] if parent_results['metadatas'] else {}\n",
    "                    )\n",
    "                    expanded_docs.append(parent_doc)\n",
    "                    parent_ids_seen.add(parent_id)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not retrieve parent {parent_id}: {e}\")\n",
    "    \n",
    "    return expanded_docs\n",
    "\n",
    "\n",
    "# ========== CELL 3: Advanced Retrieval with Hierarchical Reranking ==========\n",
    "def retrieve_and_rerank_hierarchical(\n",
    "    query: str,\n",
    "    vector_store,\n",
    "    k_initial: int = 10,\n",
    "    top_n_final: int = 5,\n",
    "    expand_parents: bool = True,\n",
    "    use_parent_boost: bool = True\n",
    ") -> Tuple[List[Document], Dict]:\n",
    "    \"\"\"\n",
    "    Complete retrieval pipeline dengan hierarchical support dan reranking\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (reranked_documents, metrics_dict)\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🔍 HIERARCHICAL RETRIEVAL + RERANKING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Initial retrieval\n",
    "    print(f\"\\n📥 Step 1: Initial retrieval (k={k_initial})\")\n",
    "    start = time.time()\n",
    "    initial_results = vector_store.similarity_search(query, k=k_initial)\n",
    "    retrieval_time = time.time() - start\n",
    "    metrics['retrieval_time'] = retrieval_time\n",
    "    metrics['initial_count'] = len(initial_results)\n",
    "    \n",
    "    print(f\"   ✅ Retrieved {len(initial_results)} chunks\")\n",
    "    print(f\"   ⏱️  Time: {retrieval_time*1000:.2f}ms\")\n",
    "    \n",
    "    # Analyze chunk types\n",
    "    chunk_types = {}\n",
    "    for doc in initial_results:\n",
    "        chunk_type = doc.metadata.get('chunk_type', 'unknown')\n",
    "        chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1\n",
    "    \n",
    "    print(f\"   📊 Chunk types: {chunk_types}\")\n",
    "    \n",
    "    # Step 2: Expand to parents (optional)\n",
    "    if expand_parents:\n",
    "        print(f\"\\n🔼 Step 2: Expanding to parent chunks\")\n",
    "        start = time.time()\n",
    "        expanded_results = expand_to_parent_chunks(initial_results, vector_store)\n",
    "        expand_time = time.time() - start\n",
    "        metrics['expand_time'] = expand_time\n",
    "        metrics['expanded_count'] = len(expanded_results)\n",
    "        \n",
    "        print(f\"   ✅ Expanded to {len(expanded_results)} chunks (includes parents)\")\n",
    "        print(f\"   ⏱️  Time: {expand_time*1000:.2f}ms\")\n",
    "        \n",
    "        documents_to_rerank = expanded_results\n",
    "    else:\n",
    "        documents_to_rerank = initial_results\n",
    "        metrics['expand_time'] = 0\n",
    "        metrics['expanded_count'] = len(initial_results)\n",
    "    \n",
    "    # Step 3: Rerank\n",
    "    print(f\"\\n🎯 Step 3: Reranking to top {top_n_final}\")\n",
    "    start = time.time()\n",
    "    reranked_results = rerank_documents_hierarchical(\n",
    "        query=query,\n",
    "        documents=documents_to_rerank,\n",
    "        top_n=top_n_final,\n",
    "        use_parent=True,\n",
    "        parent_boost=0.1 if use_parent_boost else 0\n",
    "    )\n",
    "    rerank_time = time.time() - start\n",
    "    metrics['rerank_time'] = rerank_time\n",
    "    metrics['final_count'] = len(reranked_results)\n",
    "    \n",
    "    print(f\"   ⏱️  Time: {rerank_time*1000:.2f}ms\")\n",
    "    \n",
    "    # Total time\n",
    "    total_time = retrieval_time + metrics['expand_time'] + rerank_time\n",
    "    metrics['total_time'] = total_time\n",
    "    \n",
    "    print(f\"\\n⏱️  TOTAL TIME: {total_time*1000:.2f}ms\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return reranked_results, metrics\n",
    "\n",
    "\n",
    "# ========== CELL 4: Display Results Function ==========\n",
    "def display_reranked_results(documents: List[Document], query: str = None):\n",
    "    \"\"\"\n",
    "    Display reranked results dengan hierarchy information\n",
    "    \"\"\"\n",
    "    if query:\n",
    "        print(f\"\\n❓ Query: '{query}'\")\n",
    "    \n",
    "    print(f\"\\n📚 Top {len(documents)} Results:\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        # Extract metadata\n",
    "        page = doc.metadata.get('page', 'N/A')\n",
    "        chunk_type = doc.metadata.get('chunk_type', 'unknown')\n",
    "        chunk_id = doc.metadata.get('chunk_id', 'N/A')\n",
    "        parent_id = doc.metadata.get('parent_id', None)\n",
    "        relevance_score = doc.metadata.get('relevance_score', 0)\n",
    "        raw_score = doc.metadata.get('raw_score', 0)\n",
    "        \n",
    "        # Header\n",
    "        print(f\"\\n{i}. 📄 Page {page if page == 'N/A' else page+1} | Type: {chunk_type.upper()}\")\n",
    "        print(f\"   ID: {chunk_id}\")\n",
    "        if parent_id:\n",
    "            print(f\"   Parent ID: {parent_id}\")\n",
    "        \n",
    "        # Scores\n",
    "        print(f\"   🎯 Relevance Score: {relevance_score:.4f} (raw: {raw_score:.4f})\")\n",
    "        \n",
    "        # Content preview\n",
    "        content_preview = doc.page_content[:200] if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"   📝 Content:\\n   {content_preview}...\")\n",
    "        \n",
    "        # Hierarchy indicator\n",
    "        if chunk_type == 'parent':\n",
    "            print(f\"   🔼 [PARENT CHUNK - Contains broader context]\")\n",
    "        elif chunk_type == 'child':\n",
    "            print(f\"   🔽 [CHILD CHUNK - Focused content]\")\n",
    "        \n",
    "        print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e051b3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🧪 TEST: HIERARCHICAL RETRIEVAL + RERANKING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "🔍 HIERARCHICAL RETRIEVAL + RERANKING\n",
      "======================================================================\n",
      "\n",
      "📥 Step 1: Initial retrieval (k=10)\n",
      "   ✅ Retrieved 10 chunks\n",
      "   ⏱️  Time: 22.10ms\n",
      "   📊 Chunk types: {'unknown': 10}\n",
      "\n",
      "🔼 Step 2: Expanding to parent chunks\n",
      "   ✅ Expanded to 10 chunks (includes parents)\n",
      "   ⏱️  Time: 0.00ms\n",
      "\n",
      "🎯 Step 3: Reranking to top 5\n",
      "🔍 Reranking 10 documents...\n",
      "✅ Reranking complete! Top score: 0.5135\n",
      "   ⏱️  Time: 901.71ms\n",
      "\n",
      "⏱️  TOTAL TIME: 923.81ms\n",
      "======================================================================\n",
      "\n",
      "❓ Query: 'Apa itu overfitting?'\n",
      "\n",
      "📚 Top 5 Results:\n",
      "\n",
      "======================================================================\n",
      "\n",
      "1. 📄 Page N/A | Type: UNKNOWN\n",
      "   ID: N/A\n",
      "   🎯 Relevance Score: 0.5135 (raw: 0.5135)\n",
      "   📝 Content:\n",
      "   Normalisasi data: JST sangat sensitif terhadap perbedaan skala antar fitur, oleh karena itu,\n",
      "sangat disarankan untuk menormalisasi data sebelum melatih model\n",
      "Inisialisasi bobot: Inisialisasi bobot yan...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "2. 📄 Page N/A | Type: UNKNOWN\n",
      "   ID: N/A\n",
      "   🎯 Relevance Score: -1.4964 (raw: -1.4964)\n",
      "   📝 Content:\n",
      "   Ensemble methods direkomendasikan dalam situasi berikut:  \n",
      "40  \n",
      "Ketika model dasar mengalami overfitting atau underfitting\n",
      "Ketika model dasar memiliki kecenderungan yang berbeda dan varians\n",
      "Ketika men...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "3. 📄 Page N/A | Type: UNKNOWN\n",
      "   ID: N/A\n",
      "   🎯 Relevance Score: -2.8662 (raw: -2.8662)\n",
      "   📝 Content:\n",
      "   Untuk menemukan nilai k yang optimal, kita bisa menggunakan metode cross-validation.\n",
      "Berikut contoh cara melakukannya dengan menggunakan library scikit-learn:  \n",
      "17  \n",
      "Dengan menggunakan GridSearchCV, k...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "4. 📄 Page N/A | Type: UNKNOWN\n",
      "   ID: N/A\n",
      "   🎯 Relevance Score: -2.8796 (raw: -2.8796)\n",
      "   📝 Content:\n",
      "   MultinomialNB memiliki beberapa parameter utama:  \n",
      "1. alpha: Parameter penghalusan Laplace atau Lidstone, digunakan untuk menghindari\n",
      "probabilitas nol pada likelihood. Nilai default adalah 1.0, dan ni...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "5. 📄 Page N/A | Type: UNKNOWN\n",
      "   ID: N/A\n",
      "   🎯 Relevance Score: -4.2581 (raw: -4.2581)\n",
      "   📝 Content:\n",
      "   Beberapa parameter yang perlu diperhatikan saat melakukan hyperparameter tuning pada\n",
      "JST meliputi:  \n",
      "hidden_layer_sizes: Jumlah dan ukuran lapisan tersembunyi dalam JST\n",
      "activation: Fungsi aktivasi yan...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "📊 METRICS:\n",
      "======================================================================\n",
      "   • retrieval_time: 22.10ms\n",
      "   • initial_count: 10\n",
      "   • expand_time: 0.00ms\n",
      "   • expanded_count: 10\n",
      "   • rerank_time: 901.71ms\n",
      "   • final_count: 5\n",
      "   • total_time: 923.81ms\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== CELL 5: Test Hierarchical Retrieval + Reranking ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🧪 TEST: HIERARCHICAL RETRIEVAL + RERANKING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test query\n",
    "test_query = \"Apa itu overfitting?\"\n",
    "\n",
    "# Retrieve and rerank\n",
    "reranked_docs, metrics = retrieve_and_rerank_hierarchical(\n",
    "    query=test_query,\n",
    "    vector_store=vector_store,\n",
    "    k_initial=10,           # Retrieve 10 chunks initially\n",
    "    top_n_final=5,          # Return top 5 after reranking\n",
    "    expand_parents=True,    # Include parent chunks\n",
    "    use_parent_boost=True   # Boost parent chunk scores\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display_reranked_results(reranked_docs, query=test_query)\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\n📊 METRICS:\")\n",
    "print(\"=\"*70)\n",
    "for key, value in metrics.items():\n",
    "    if 'time' in key:\n",
    "        print(f\"   • {key}: {value*1000:.2f}ms\")\n",
    "    else:\n",
    "        print(f\"   • {key}: {value}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c2998a",
   "metadata": {},
   "source": [
    "## <h4><b>Delete Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ff944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Koleksi 'machine_learning_modules' berhasil dihapus dari database.\n"
     ]
    }
   ],
   "source": [
    "# import chromadb\n",
    "\n",
    "# try:\n",
    "#     # Inisialisasi klien Chroma yang terhubung ke folder persistensi\n",
    "#     client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIRECTORY)\n",
    "\n",
    "#     # Hapus koleksi berdasarkan nama\n",
    "#     client.delete_collection(name=COLLECTION_NAME)\n",
    "    \n",
    "#     print(f\"✅ Koleksi '{COLLECTION_NAME}' berhasil dihapus dari database.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Gagal menghapus koleksi: {e}\")\n",
    "#     print(\"Mungkin koleksi tidak ditemukan atau database belum ada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466af15",
   "metadata": {},
   "source": [
    "# <h3><b>Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13ec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prompt template created\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Kamu adalah asisten AI yang membantu menjawab pertanyaan berdasarkan konteks yang diberikan.\n",
    "\n",
    "Konteks:\n",
    "{context}\n",
    "\n",
    "Pertanyaan: {question}\n",
    "\n",
    "Instruksi:\n",
    "- Jawab berdasarkan konteks yang diberikan\n",
    "- Jika tidak ada informasi di konteks, katakan \"Maaf, saya tidak menemukan informasi tersebut dalam dokumen\"\n",
    "- Berikan jawaban yang jelas dan ringkas\n",
    "- Gunakan bahasa Indonesia yang baik\n",
    "\n",
    "Jawaban:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(\"✅ Prompt template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9521646e",
   "metadata": {},
   "source": [
    "# <h3><b>RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af9eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG Chain created!\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"Format dokumen untuk context\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain menggunakan LCEL (LangChain Expression Language)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ RAG Chain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2cfc3",
   "metadata": {},
   "source": [
    "# <h3><b>Query Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfed12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    \"\"\"Fungsi untuk bertanya ke chatbot\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"❓ Pertanyaan: {question}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get answer\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"\\n💬 Jawaban:\\n{answer}\")\n",
    "    \n",
    "    # Get source documents\n",
    "    docs = retriever.invoke(question)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📚 Sumber Dokumen:\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"\\n{i}. {doc.page_content[:150]}...\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6c1a5",
   "metadata": {},
   "source": [
    "# <h3><b>Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d645ea88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melakukan pencarian untuk query: 'Apa itu overfitting?'\n"
     ]
    }
   ],
   "source": [
    "# --- 3. LAKUKAN PENCARIAN ---\n",
    "query = \"Apa itu overfitting?\"\n",
    "print(f\"\\nMelakukan pencarian untuk query: '{query}'\")\n",
    "\n",
    "relevant_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ad772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ditemukan 5 dokumen relevan:\n",
      "\n",
      "--- Dokumen #1 ---\n",
      "Metadata: {'Header 3': '**9.5 Kasus-kasus yang Direkomendasikan**', 'Header 1': '**Bab 9: Ensemble Methods:** **Meningkatkan Performa dengan** **Metode Gabungan**'}\n",
      "Konten: Ensemble methods direkomendasikan dalam situasi berikut:  \n",
      "40  \n",
      "Ketika model dasar mengalami overfitting atau underfitting\n",
      "Ketika model dasar memiliki kecenderungan yang berbeda dan varians\n",
      "Ketika menghadapi masalah klasifikasi yang kompleks atau data yang sangat tidak seimbang...\n",
      "\n",
      "--- Dokumen #2 ---\n",
      "Metadata: {'Header 3': '**8.7 Hal-hal yang Harus Diperhatikan**', 'Header 1': '**Bab 8: Neural Networks - Klasifikasi** **dengan Menggunakan Jaringan Saraf** **Tiruan**'}\n",
      "Konten: Normalisasi data: JST sangat sensitif terhadap perbedaan skala antar fitur, oleh karena itu,\n",
      "sangat disarankan untuk menormalisasi data sebelum melatih model\n",
      "Inisialisasi bobot: Inisialisasi bobot yang baik bisa membantu konvergensi model lebih  \n",
      "cepat\n",
      "Overfitting: JST cenderung mudah overfitting, jadi pastikan untuk menggunakan\n",
      "regularisasi dan validasi silang untuk mengurangi risiko overfitting\n",
      "Arsitektur JST: Memilih arsitektur yang tepat sangat penting, seperti jumlah lapisan, jumlah\n",
      "neuron, dan fungsi aktivasi...\n",
      "\n",
      "--- Dokumen #3 ---\n",
      "Metadata: {'Header 1': '**Bab 3: K-Nearest Neighbors: Algoritma** **Berbasis Jarak untuk Klasifikasi**', 'Header 3': '**3.6 Hal-Hal yang Harus Diperhatikan**'}\n",
      "Konten: Penting untuk menormalisasi fitur sebelum menggunakan KNN, karena fitur dengan\n",
      "rentang yang lebih besar akan memiliki pengaruh yang lebih besar pada jarak antar poin  \n",
      "data.  \n",
      "KNN sensitif terhadap jumlah tetangga (k) yang digunakan. Nilai k yang terlalu kecil bisa\n",
      "menyebabkan overfitting, sedangkan k yang terlalu besar bisa menyebabkan underfitting.\n",
      "Gunakan metode seperti cross-validation untuk menemukan nilai k yang optimal.\n",
      "KNN cenderung lambat untuk data dengan jumlah fitur yang besar, karena perhitungan\n",
      "jarak menjadi lebih kompleks....\n",
      "\n",
      "--- Dokumen #4 ---\n",
      "Metadata: {'Header 1': '**Bab 3: K-Nearest Neighbors: Algoritma** **Berbasis Jarak untuk Klasifikasi**', 'Header 3': '**3.8 Optimasi KNN dengan Cross-Validation**'}\n",
      "Konten: Untuk menemukan nilai k yang optimal, kita bisa menggunakan metode cross-validation.\n",
      "Berikut contoh cara melakukannya dengan menggunakan library scikit-learn:  \n",
      "17  \n",
      "Dengan menggunakan GridSearchCV, kita bisa mencari nilai k dan parameter lain yang\n",
      "optimal untuk meningkatkan performa model KNN. Selalu penting untuk melakukan tuning\n",
      "dan validasi model untuk menghindari overfitting atau underfitting....\n",
      "\n",
      "--- Dokumen #5 ---\n",
      "Metadata: {'Header 3': '**7.6 Penjelasan Parameter dan Hyperparameter Tuning**', 'Header 1': '**Bab 7: Naive Bayes: Mengaplikasikan** **Teorema Bayes dalam Klasifikasi**'}\n",
      "Konten: MultinomialNB memiliki beberapa parameter utama:  \n",
      "1. alpha: Parameter penghalusan Laplace atau Lidstone, digunakan untuk menghindari\n",
      "probabilitas nol pada likelihood. Nilai default adalah 1.0, dan nilai yang lebih besar akan\n",
      "mengurangi overfitting.  \n",
      "2. fit_prior: Parameter boolean yang menentukan apakah akan menggunakan probabilitas\n",
      "prior yang dihitung dari data atau probabilitas prior yang seragam. Nilai default adalah  \n",
      "True.  \n",
      "3. class_prior: Array probabilitas prior yang bisa diberikan secara manual jika fit_prior\n",
      "adalah False.  \n",
      "Untuk hyperparameter tuning, kamu bisa menggunakan metode seperti GridSearchCV atau\n",
      "RandomizedSearchCV dari scikit-learn untuk mencari kombinasi terbaik dari parameter....\n"
     ]
    }
   ],
   "source": [
    "# --- 4. TAMPILKAN HASIL ---\n",
    "if relevant_docs:\n",
    "    print(f\"\\nDitemukan {len(relevant_docs)} dokumen relevan:\")\n",
    "    for i, doc in enumerate(relevant_docs):\n",
    "        print(f\"\\n--- Dokumen #{i+1} ---\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "        # print(f\"Konten: {doc.page_content[:300]}...\")\n",
    "        print(f\"Konten: {doc.page_content}...\")\n",
    "else:\n",
    "    print(\"Tidak ada dokumen relevan ditemukan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184a527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "❓ Pertanyaan: Apa itu Model Klasifikasi?\n",
      "============================================================\n",
      "\n",
      "💬 Jawaban:\n",
      "Model klasifikasi adalah jenis model pembelajaran mesin yang digunakan untuk memetakan data input ke dalam satu atau beberapa kategori (kelas) yang telah ditentukan. Model ini mempelajari pola atau hubungan antara fitur‑fitur data (baik numerik maupun yang telah dikonversi menjadi numerik) dan label target, sehingga dapat memprediksi kelas dari data baru. Contohnya termasuk SVM, decision tree, logistic regression, dan lain‑lain, yang dapat diterapkan pada masalah klasifikasi binomial maupun multi‑kelas.\n",
      "\n",
      "============================================================\n",
      "📚 Sumber Dokumen:\n",
      "\n",
      "1. Setelah mempelajari berbagai model klasifikasi dan metrik evaluasi yang relevan, kita akan\n",
      "membahas tentang praktik terbaik dalam menerapkan model kla...\n",
      "\n",
      "2. SVM direkomendasikan untuk kasus-kasus berikut:  \n",
      "1. Klasifikasi binomial atau multi-kelas.  \n",
      "20  \n",
      "2. Data dengan fitur numerik atau kategorikal (yang...\n",
      "\n",
      "3. Berikut adalah tiga contoh kasus nyata yang menunjukkan bagaimana model klasifikasi bisa\n",
      "diterapkan dalam berbagai situasi menggunakan data asli yang ...\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model klasifikasi adalah jenis model pembelajaran mesin yang digunakan untuk memetakan data input ke dalam satu atau beberapa kategori (kelas) yang telah ditentukan. Model ini mempelajari pola atau hubungan antara fitur‑fitur data (baik numerik maupun yang telah dikonversi menjadi numerik) dan label target, sehingga dapat memprediksi kelas dari data baru. Contohnya termasuk SVM, decision tree, logistic regression, dan lain‑lain, yang dapat diterapkan pada masalah klasifikasi binomial maupun multi‑kelas.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Apa itu Model Klasifikasi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdadcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "❓ Pertanyaan: Siapa presiden Indonesia?\n",
      "============================================================\n",
      "\n",
      "💬 Jawaban:\n",
      "Maaf, saya tidak menemukan informasi tersebut dalam dokumen.\n",
      "\n",
      "============================================================\n",
      "📚 Sumber Dokumen:\n",
      "\n",
      "1. Akurasi adalah metrik yang paling sederhana dan intuitif. Akurasi menggambarkan\n",
      "seberapa sering model membuat prediksi yang benar.  \n",
      "Akurasi = (Jumlah...\n",
      "\n",
      "2. Contoh: Mengklasifikasikan jenis kelamin berdasarkan tinggi, berat, dan ukuran sepatu.  \n",
      "24...\n",
      "\n",
      "3. JST cocok untuk digunakan pada kasus-kasus berikut:  \n",
      "1. Klasifikasi gambar dan pengenalan pola\n",
      "2. Analisis teks dan pemrosesan bahasa alami  \n",
      "36  \n",
      "3....\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Maaf, saya tidak menemukan informasi tersebut dalam dokumen.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Siapa presiden Indonesia?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
